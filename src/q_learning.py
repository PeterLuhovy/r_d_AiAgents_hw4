import numpy as np

class QLearningAgent:
    def __init__(self, debug=False):
        """
        Vytv√°rame 'mozog' robota ktor√Ω sa nauƒç√≠ chodi≈•
        """
        self.debug = debug
        
        # "OSOBNOS≈§" robota - ako sa spr√°va:
        self.curiosity = 0.9        # epsilon *(zvedavos≈•)*
        self.learning_speed = 0.1   # learning rate *(r√Ωchlos≈• uƒçenia)*
        self.memory_decay = 0.99    # gamma *(ako d√¥le≈æit√© s√∫ bud√∫ce v√Ωsledky)*
        
        # DISKRETIZ√ÅCIA PARAMETROV *(nastavenie pre rozdelenie situ√°ci√≠)*
        self.bins_per_dimension = 10  # Koƒæko "ko≈°√≠kov" pre ka≈æd√∫ hodnotu stavu
        
        # HRANICE STAVOV√âHO PRIESTORU *(min/max hodnoty z hexapod_env.py)*
        self.state_bounds = {
            'x_pos': (-20, 20),      # x poz√≠cia v metroch
            'y_pos': (-20, 20),      # y poz√≠cia v metroch  
            'x_vel': (-1, 1),        # x r√Ωchlos≈• v m/s
            'y_vel': (-1, 1),        # y r√Ωchlos≈• v m/s
            'orientation': (-3.14, 3.14),    # orient√°cia v radi√°noch
            'angular_vel': (-2, 2)   # uhlov√° r√Ωchlos≈• v rad/s
        }
        
        # V√ùPOƒåET VEƒΩKOSTI Q-TABUƒΩKY
        total_states = self.bins_per_dimension ** 6  # 6 dimenzi√≠ stavu
        self.q_table = np.zeros((total_states, 3))   # 3 akcie: vpred, vƒæavo, vpravo

        if self.debug:
            print("ü§ñ === INICIALIZ√ÅCIA Q-LEARNING AGENTA ===")
            print(f"‚úÖ Zvedavos≈• (epsilon): {self.curiosity}")
            print(f"‚úÖ R√Ωchlos≈• uƒçenia: {self.learning_speed}")
            print(f"‚úÖ Pam√§≈• decay (gamma): {self.memory_decay}")
            print(f"‚úÖ Ko≈°√≠ky na dimenziu: {self.bins_per_dimension}")
            print(f"‚úÖ Q-tabuƒæka: {self.q_table.shape}, bunky: {self.q_table.size}")
            print("=" * 50)



    def discretize_state(self, continuous_state):
        """
        STATE DISCRETIZATION *(prevod plynul√Ωch ƒç√≠sel na kateg√≥rie)*
        """
        discrete_state = []
        state_names = ['x_pos', 'y_pos', 'x_vel', 'y_vel', 'orientation', 'angular_vel']
        
        for i, (state_name, value) in enumerate(zip(state_names, continuous_state)):
            min_val, max_val = self.state_bounds[state_name]
            
            # NORMALIZ√ÅCIA *(prevod na rozsah 0-1)*
            normalized = (value - min_val) / (max_val - min_val)
            normalized = max(0, min(1, normalized))  # Orezanie na 0-1
            
            # DISKRETIZ√ÅCIA *(zaradenie do kateg√≥rie 0-9)*
            discrete_value = int(normalized * (self.bins_per_dimension - 1))
            discrete_state.append(discrete_value)
        
        # KONVERZIA NA INDEX *(jedin√© ƒç√≠slo pre cel√Ω stav)*
        state_index = 0
        for i, val in enumerate(discrete_state):
            state_index += val * (self.bins_per_dimension ** i)
            
        return state_index
    
    def update(self, s_idx, a, r, s_next_idx, terminated: bool):
        """
        Q-learning update s debug v√Ωpisom.
        s_idx      = index aktu√°lneho stavu
        a          = vykonan√° akcia
        r          = odmena
        s_next_idx = index nasleduj√∫ceho stavu
        terminated = True, ak epiz√≥da skonƒçila
        """
        current_q = self.q_table[s_idx, a]

        if terminated:
            target = r
        else:
            best_next_q = np.max(self.q_table[s_next_idx])
            target = r + self.memory_decay * best_next_q

        td_error = target - current_q
        new_q = current_q + self.learning_speed * td_error
        self.q_table[s_idx, a] = new_q

        if self.debug:
            print(f"[UPDATE] s={s_idx} a={a} r={r:.4f} term={terminated}")
            if not terminated:
                print(f"  best_next_q={best_next_q:.4f}")
            print(f"  current_q={current_q:.4f} target={target:.4f} td_error={td_error:.4f} new_q={new_q:.4f}")


    def choose_action(self, state_idx):
        """
        Epsilon-greedy v√Ωber akcie:
        - s pravdepodobnos≈•ou epsilon (self.curiosity) vyber n√°hodn√∫ akciu (prieskum)
        - inak vyber akciu s najvy≈°≈°ou Q-hodnotou (exploat√°cia)
        """
        if np.random.rand() < self.curiosity:
            # n√°hodn√° akcia (prieskum)
            return np.random.randint(self.q_table.shape[1])
        else:
            # najlep≈°ia akcia podƒæa Q-tabuƒæky (exploat√°cia)
            return int(np.argmax(self.q_table[state_idx]))


# Test diskretiz√°cie
if __name__ == "__main__":
    agent = QLearningAgent()
    
    # Test s pr√≠kladn√Ωm stavom
    test_state = [5.2, -3.1, 0.5, -0.2, 1.57, 0.8]  # pr√≠klad z prostredia
    discrete_index = agent.discretize_state(test_state)
    print(f"üìä Stav {test_state} ‚Üí diskr√©tny index {discrete_index}")